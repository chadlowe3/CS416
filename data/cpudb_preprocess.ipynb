{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2bbc335-841e-4419-87e6-da42982bab1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfBoundsDatetime",
     "evalue": "cannot convert input with unit 'D'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mconversion.pyx:156\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.cast_from_unit_vectorized\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: value too large",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOutOfBoundsDatetime\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:512\u001b[0m, in \u001b[0;36m_to_datetime_with_unit\u001b[1;34m(arg, unit, name, utc, errors)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m     arr \u001b[38;5;241m=\u001b[39m cast_from_unit_vectorized(arg, unit\u001b[38;5;241m=\u001b[39munit)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutOfBoundsDatetime:\n",
      "File \u001b[1;32mconversion.pyx:160\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.cast_from_unit_vectorized\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOutOfBoundsDatetime\u001b[0m: cannot convert input 719832.0 with the unit 'D'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutOfBoundsDatetime\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 103\u001b[0m\n\u001b[0;32m     99\u001b[0m metric_data \u001b[38;5;241m=\u001b[39m clean_for_regression(metric_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_ordinal\u001b[39m\u001b[38;5;124m'\u001b[39m, metric)\n\u001b[0;32m    101\u001b[0m date_range, trend \u001b[38;5;241m=\u001b[39m calculate_regression_line(metric_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_ordinal\u001b[39m\u001b[38;5;124m'\u001b[39m], metric_data[metric])\n\u001b[1;32m--> 103\u001b[0m trend_dates \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(date_range \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m365.25\u001b[39m \u001b[38;5;241m+\u001b[39m all_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mtoordinal(), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    105\u001b[0m trend_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: trend_dates, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_trend\u001b[39m\u001b[38;5;124m'\u001b[39m: trend})\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Save to separate files\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1099\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         result \u001b[38;5;241m=\u001b[39m _convert_and_box_cache(argc, cache_array)\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         result \u001b[38;5;241m=\u001b[39m convert_listlike(argc, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m     result \u001b[38;5;241m=\u001b[39m convert_listlike(np\u001b[38;5;241m.\u001b[39marray([arg]), \u001b[38;5;28mformat\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:407\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot specify both format and unit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _to_datetime_with_unit(arg, unit, name, utc, errors)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg must be a string, datetime, list, tuple, 1-d array, or Series\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:518\u001b[0m, in \u001b[0;36m_to_datetime_with_unit\u001b[1;34m(arg, unit, name, utc, errors)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    515\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m _to_datetime_with_unit(\n\u001b[0;32m    516\u001b[0m                 arg\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m), unit, name, utc, errors\n\u001b[0;32m    517\u001b[0m             )\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OutOfBoundsDatetime(\n\u001b[0;32m    519\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot convert input with unit \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    520\u001b[0m         )\n\u001b[0;32m    522\u001b[0m arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM8[ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    523\u001b[0m tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mOutOfBoundsDatetime\u001b[0m: cannot convert input with unit 'D'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Read in CSV tables\n",
    "processor = pd.read_csv(\"./cpudb/processor.csv\")\n",
    "specint2k6 = pd.read_csv(\"./cpudb/spec_int2006.csv\")\n",
    "specint2k0 = pd.read_csv(\"./cpudb/spec_int2000.csv\")\n",
    "specint95 = pd.read_csv(\"./cpudb/spec_int1995.csv\")\n",
    "specint92 = pd.read_csv(\"./cpudb/spec_int1992.csv\")\n",
    "\n",
    "\n",
    "# Rename [processor].[id] to [processor].[processor_id]\n",
    "processor.rename(columns={'id': 'processor_id'}, inplace=True)\n",
    "\n",
    "\n",
    "# Merge spec scores\n",
    "all_data = processor.merge(specint2k6, on=\"processor_id\", suffixes=(\".proc\", \".spec_int2k6\"), how=\"outer\")\n",
    "all_data = all_data.merge(specint2k0, on=\"processor_id\", suffixes=(\".spec_int2k6\", \".spec_int2k0\"), how=\"outer\")\n",
    "all_data = all_data.merge(specint95, on=\"processor_id\", suffixes=(\".spec_int2k0\", \".spec_int95\"), how=\"outer\")\n",
    "all_data = all_data.merge(specint92, on=\"processor_id\", suffixes=(\".spec_int95\", \".spec_int92\"), how=\"outer\")\n",
    "\n",
    "# Fix missing date entries\n",
    "all_data['date'] = pd.to_datetime(all_data['date'], errors='coerce')\n",
    "\n",
    "# Sort by date\n",
    "all_data.sort_values('date', inplace=True)\n",
    "\n",
    "# Drop rows with NaT in 'date' column\n",
    "all_data.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Account for potential turbo-boost clock\n",
    "all_data.fillna({'max_clock': all_data['clock']}, inplace=True)\n",
    "#all_data['max_clock'].fillna(all_data['clock'], inplace=True)\n",
    "\n",
    "# Determine scaling factors for spec92->spec95, spec95->spec2k0, and spec2k0->spec2k6\n",
    "spec92to95 = all_data['basemean.spec_int95'] / all_data['basemean.spec_int92']\n",
    "spec95to2k0 = all_data['basemean.spec_int2k0'] / all_data['basemean.spec_int95']\n",
    "spec2k0to2k6 = all_data['basemean.spec_int2k6'] / all_data['basemean.spec_int2k0']\n",
    "\n",
    "all_data.fillna({'basemean.spec_int95': spec92to95.mean() * all_data['basemean.spec_int92']}, inplace=True)\n",
    "#all_data['basemean.spec_int95'].fillna(spec92to95.mean() * all_data['basemean.spec_int92'], inplace=True)\n",
    "\n",
    "all_data.fillna({'basemean.spec_int2k0': spec95to2k0.mean() * all_data['basemean.spec_int95']}, inplace=True)\n",
    "#all_data['basemean.spec_int2k0'].fillna(spec95to2k0.mean() * all_data['basemean.spec_int95'], inplace=True)\n",
    "\n",
    "all_data.fillna({'basemean.spec_int2k6': spec2k0to2k6.mean() * all_data['basemean.spec_int2k0']}, inplace=True)\n",
    "#all_data['basemean.spec_int2k6'].fillna(spec2k0to2k6.mean() * all_data['basemean.spec_int2k0'], inplace=True)\n",
    "\n",
    "# Performance\n",
    "all_data['perfnorm'] = all_data['basemean.spec_int2k6'] / all_data['tdp']\n",
    "\n",
    "# Find the scaling factors\n",
    "scaleclk = all_data['max_clock'].min()\n",
    "scaletrans = all_data['transistors'].min()\n",
    "scaletdp = all_data['tdp'].min()\n",
    "scaleperf = all_data['basemean.spec_int2k6'].min()\n",
    "scaleperfnorm = all_data['perfnorm'].min()\n",
    "\n",
    "# Calculate relative scaling\n",
    "all_data['rel_transistors'] = all_data['transistors'] / scaletrans\n",
    "all_data['rel_max_clock'] = all_data['max_clock'] / scaleclk\n",
    "all_data['rel_tdp'] = all_data['tdp'] / scaletdp\n",
    "all_data['rel_perf'] = all_data['basemean.spec_int2k6'] / scaleperf\n",
    "all_data['rel_perfnorm'] = all_data['perfnorm'] / scaleperfnorm\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate regression line\n",
    "def clean_for_regression(df, x_col, y_col):\n",
    "    df = df.dropna(subset=[x_col, y_col])\n",
    "    df = df[np.isfinite(df[x_col]) & np.isfinite(df[y_col])]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Convert dates to ordinal for regression\n",
    "all_data['date_ordinal'] = (all_data['date'] - all_data['date'].min()).dt.days / 365.25\n",
    "\n",
    "# Metrics to process\n",
    "#metrics = ['rel_transistors', 'rel_max_clock', 'rel_tdp', 'rel_perf', 'rel_perfnorm']\n",
    "metrics = ['rel_transistors']\n",
    "\n",
    "\n",
    "\n",
    "def calculate_regression_line(x, y):\n",
    "    model = LinearRegression()\n",
    "    x_reshape = x.values.reshape(-1, 1)\n",
    "    model.fit(x_reshape, y)\n",
    "    date_range = np.linspace(x.min(), x.max(), len(x))\n",
    "    date_range_reshape = date_range.reshape(-1, 1)\n",
    "    trend = model.predict(date_range_reshape)\n",
    "    return date_range, trend\n",
    "\n",
    "# Process each metric\n",
    "for metric in metrics:\n",
    "    metric_data = all_data[['date', 'date_ordinal', metric]].dropna()\n",
    "    metric_data = clean_for_regression(metric_data, 'date_ordinal', metric)\n",
    "    \n",
    "    date_range, trend = calculate_regression_line(metric_data['date_ordinal'], metric_data[metric])\n",
    "    \n",
    "    trend_dates = pd.to_datetime(date_range * 365.25 + all_data['date'].min().toordinal(), unit='D', origin='unix')\n",
    "    \n",
    "    trend_df = pd.DataFrame({'date': trend_dates, f'{metric}_trend': trend})\n",
    "    \n",
    "    # Save to separate files\n",
    "    output_data = pd.merge(metric_data[['date', metric]], trend_df, on='date', how='outer')\n",
    "    output_data.to_csv(f'./cpu_performance_trend_{metric}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
